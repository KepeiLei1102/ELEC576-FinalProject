{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LmNcPhwTJ07K"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from torchvision import datasets\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy.random\n",
    "from numpy.random import randint\n",
    "import time\n",
    "# from ray import tune\n",
    "# from ray.tune import JupyterNotebookReporter\n",
    "# from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9hYSA2TjhoA4"
   },
   "outputs": [],
   "source": [
    "root = 'C:/Users/Joseph/Downloads/'\n",
    "X_stft_train = np.load(root+'X_stft_train_nfft=1024.npy')\n",
    "# n_mfcc, ceps_length, n = X_train_mfcc.shape\n",
    "y_train = np.load(root+'train_labels.npy').squeeze()\n",
    "# X_test_mfcc = np.load(root+'test_mfcc_nmfcc=5_nfft=8192_hl=512_3obsPerWav.npy')\n",
    "# n_test = X_test_mfcc.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BsaT5N5tPlJ1"
   },
   "outputs": [],
   "source": [
    "class stftDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Path to the label array.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.labels = np.load(root+'train_labels.npy')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "#         mfcc_name = os.path.join(self.root_dir, str(idx+1)+'.npy')\n",
    "#         #reformat mfcc matrix to be closer to square:\n",
    "#         mfcc = np.load(mfcc_name).reshape((28,35))\n",
    "#         mfcc = np.expand_dims(mfcc,axis=2)\n",
    "#         genre = np.floor(np.floor((float(idx))/3)/70)\n",
    "#         genre = np.array([genre]).astype('float')\n",
    "        stft = X_stft_train[idx]\n",
    "        stft = np.expand_dims(stft, axis=2)\n",
    "        label = np.array([y_train[idx]]).astype('float')\n",
    "        sample = {'stft': stft, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        stft, label = sample['stft'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        stft = stft.transpose((2, 0, 1))\n",
    "        return {'stft': torch.from_numpy(stft),\n",
    "                'label': torch.from_numpy(label)}\n",
    "\n",
    "stft_dataset = stftDataset(root=root, root_dir=root, transform = ToTensor())\n",
    "test_prop = 0.1\n",
    "train_prop = 1-test_prop\n",
    "n = len(stft_dataset)\n",
    "trainset, testset = random_split(stft_dataset, [int(n*train_prop), n-int(n*train_prop)], generator=torch.Generator().manual_seed(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=50, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3763, 513, 431)\n"
     ]
    }
   ],
   "source": [
    "print(X_stft_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "351VTfTLLns7",
    "outputId": "69a7442a-c5bc-4e44-946f-c52fb7d01475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0887, 0.5882, 0.7319],\n",
      "        [0.6297, 0.0181, 0.9369],\n",
      "        [0.0336, 0.7992, 0.3280],\n",
      "        [0.7274, 0.1177, 0.6452],\n",
      "        [0.8996, 0.1115, 0.2332]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HOSW8Rh4DBo-"
   },
   "outputs": [],
   "source": [
    "def train(net, n_epochs, trainloader, use_gpu, step_size, momentum, show_loss_every):\n",
    "    if use_gpu:\n",
    "        net = net.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=step_size, momentum=momentum)\n",
    "#     optimizer = optim.Adam(net.parameters(), lr=step_size)\n",
    "\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a dictionary of {'stft': stft, 'label': label}\n",
    "            inputs = data['stft']\n",
    "            labels = data['label'].squeeze()\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs.float())\n",
    "            loss = criterion(outputs, labels.long())\n",
    "            loss.backward()  #Actually calculate gradient\n",
    "            optimizer.step() #Update the weights\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % show_loss_every == 0:    # print every show_loss_every mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / show_loss_every))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "dQgbEC3xDGM2"
   },
   "outputs": [],
   "source": [
    "#Calculate test error\n",
    "def test(net, testloader, use_gpu):\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            #Load data\n",
    "            inputs = data['stft']\n",
    "            labels = data['label'].squeeze()\n",
    "            if use_gpu:\n",
    "                inputs = inputs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(inputs.float())\n",
    "            # print(labels)\n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            loss(outputs, labels.long())\n",
    "\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    print(prediction)\n",
    "    print('Accuracy: %d %%' % (\n",
    "        100 * correct / total))\n",
    "    # return outputs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cltrLKlBhLf",
    "outputId": "63abe725-e764-48d6-e3f9-86c1b8a723b1"
   },
   "outputs": [],
   "source": [
    "# prediction = test(net, testloader, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A3OSYi-vCWdh",
    "outputId": "e0ffb4f9-e528-4725-c22b-5317085721fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "n_hidden_layers = 3\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    '''\n",
    "    Convolutional Neural Network\n",
    "    '''\n",
    "    #hyperparams: activation, optimization algorithm, max_channels, momentum, step_size, stride\n",
    "    def __init__(self, n_hidden_layers=n_hidden_layers, activation=nn.ReLU(), stride=1, dim=(3,3), \n",
    "                 pool_stride=(1,1), pool_dim=(1,1), use_batchnorm=True, use_xavier=True, max_channels=40, max_pool = False):\n",
    "        super().__init__()\n",
    "#         channel_dims = np.linspace(1, max_channels, n_hidden_layers + 1, dtype=int)\n",
    "        channel_dims = np.concatenate([np.array([1]), np.repeat(max_channels, n_hidden_layers+1)])\n",
    "        \n",
    "        #Hidden layers\n",
    "        modules = []\n",
    "#         image_size = [n_mfcc, ceps_length] # inital image size\n",
    "        image_size = [513, 431]\n",
    "        for i in range(n_hidden_layers):\n",
    "          # print(image_size)\n",
    "          dim0 = dim[0]\n",
    "          dim1 = dim[1]\n",
    "          stride0 = stride[0]\n",
    "          stride1 = stride[1]\n",
    "          modules.append(nn.Conv2d(channel_dims[i], channel_dims[i+1], (dim0,dim1), stride))\n",
    "          image_size[0] = int((image_size[0] - dim0) / stride0 + 1) #Image size after the convolution\n",
    "          image_size[1] = int((image_size[1] - dim1) / stride1 + 1)\n",
    "            \n",
    "          #Batch norm\n",
    "          if use_batchnorm:\n",
    "              modules.append(nn.BatchNorm2d(channel_dims[i+1]))\n",
    "          \n",
    "          #Activation\n",
    "          modules.append(activation)\n",
    "          \n",
    "          #Max pool\n",
    "          if max_pool:\n",
    "            pool_dim0 = pool_dim[0]\n",
    "            pool_dim1 = pool_dim[1]\n",
    "            pad0 = 0\n",
    "            pad1 = 0\n",
    "            pool_stride0 = pool_stride[0]\n",
    "            pool_stride1 = pool_stride[1]\n",
    "            modules.append(nn.MaxPool2d((pool_dim0, pool_dim1), (pool_stride0, pool_stride1), (pad0, pad1)))\n",
    "            image_size[0] = int((image_size[0]+2*pad0 - pool_dim0) / pool_stride0 + 1) #Image size after max pooling\n",
    "            image_size[1] = int((image_size[1]+2*pad1 - pool_dim1) / pool_stride1 + 1)\n",
    "        # print(image_size)\n",
    "            \n",
    "        #Dropout\n",
    "        # if dropout > 0 and dropout < 1:\n",
    "        #     modules.append(nn.Dropout2d(dropout))\n",
    "        # print(image_size)\n",
    "        #Last layer\n",
    "        modules.append(nn.Flatten())\n",
    "        input_dim  = max_channels*image_size[0]*image_size[1]\n",
    "        output_dim = np.min([int(input_dim/2), 50])\n",
    "        modules.append(nn.Linear(input_dim, output_dim))\n",
    "        modules.append(activation)\n",
    "        modules.append(nn.Linear(output_dim, 10))\n",
    "        modules.append(torch.nn.Softmax())\n",
    "        \n",
    "        #Concatenate\n",
    "        self.layers = nn.Sequential(*modules)\n",
    "        \n",
    "        #Initialize weights\n",
    "        if use_xavier:\n",
    "            self.layers.apply(self.init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Forward pass'''\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_normal_(m.weight)\n",
    "            m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rixRyKyol-R9",
    "outputId": "2e84623f-2940-45da-9861-78e793f1f019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (layers): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU()\n",
       "    (11): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=False)\n",
       "    (12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=False)\n",
       "    (16): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=(0, 0), dilation=1, ceil_mode=False)\n",
       "    (20): Flatten(start_dim=1, end_dim=-1)\n",
       "    (21): Linear(in_features=9856, out_features=50, bias=True)\n",
       "    (22): ReLU()\n",
       "    (23): Linear(in_features=50, out_features=10, bias=True)\n",
       "    (24): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN(n_hidden_layers=5, activation=nn.ReLU(), stride=(1,1), dim=(3,3), pool_stride=(2,2), pool_dim=(2,2), use_batchnorm=True, use_xavier=True, max_channels=64, max_pool = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "TTMNQ0GdM0RE",
    "outputId": "46a776e4-7145-44b8-e1af-5c308ca93aaa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Determine whether CNN is working\n",
    "\n",
    "#High level inputs\n",
    "use_gpu         = False\n",
    "seed            = 0\n",
    "\n",
    "#Training inputs\n",
    "batch_size      = 10\n",
    "n_epochs        = 1\n",
    "step_size       = 0.01\n",
    "momentum        = 0.9\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#Network architecture inputs\n",
    "n_hidden_layers = 3\n",
    "use_batchnorm   = True\n",
    "use_xavier      = True\n",
    "net = CNN(n_hidden_layers=n_hidden_layers, activation=nn.ReLU(), stride=(1,1), dim=(3,3), pool_stride=(2,2), \n",
    "          pool_dim=(2,2), use_batchnorm=True, use_xavier=True, max_channels=12, max_pool = False)\n",
    "\n",
    "#Visualization inputs\n",
    "show_loss_every = 20\n",
    "net = train(net, n_epochs, trainloader, use_gpu, step_size, momentum, show_loss_every)\n",
    "test(net, testloader, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K--z9UuEF81M",
    "outputId": "8e576f1b-413f-4fd2-f742-234e74249890"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.278\n",
      "[1,   100] loss: 2.246\n",
      "[1,   150] loss: 2.218\n",
      "[1,   200] loss: 2.218\n",
      "[2,    50] loss: 2.226\n",
      "[2,   100] loss: 2.185\n",
      "[2,   150] loss: 2.174\n",
      "[2,   200] loss: 2.196\n",
      "[3,    50] loss: 2.164\n",
      "[3,   100] loss: 2.204\n",
      "[3,   150] loss: 2.157\n",
      "[3,   200] loss: 2.200\n",
      "[4,    50] loss: 2.148\n",
      "[4,   100] loss: 2.153\n",
      "[4,   150] loss: 2.171\n",
      "[4,   200] loss: 2.167\n",
      "[5,    50] loss: 2.123\n",
      "[5,   100] loss: 2.129\n",
      "[5,   150] loss: 2.126\n",
      "[5,   200] loss: 2.121\n",
      "[6,    50] loss: 2.085\n",
      "[6,   100] loss: 2.105\n",
      "[6,   150] loss: 2.128\n",
      "[6,   200] loss: 2.142\n",
      "[7,    50] loss: 2.074\n",
      "[7,   100] loss: 2.100\n",
      "[7,   150] loss: 2.098\n",
      "[7,   200] loss: 2.098\n",
      "[8,    50] loss: 2.090\n",
      "[8,   100] loss: 2.066\n",
      "[8,   150] loss: 2.084\n",
      "[8,   200] loss: 2.093\n",
      "[9,    50] loss: 2.033\n",
      "[9,   100] loss: 2.068\n",
      "[9,   150] loss: 2.053\n",
      "[9,   200] loss: 2.071\n",
      "[10,    50] loss: 2.052\n",
      "[10,   100] loss: 2.051\n",
      "[10,   150] loss: 2.069\n",
      "[10,   200] loss: 2.075\n",
      "[11,    50] loss: 2.051\n",
      "[11,   100] loss: 2.035\n",
      "[11,   150] loss: 2.047\n",
      "[11,   200] loss: 2.042\n",
      "[12,    50] loss: 2.037\n",
      "[12,   100] loss: 2.000\n",
      "[12,   150] loss: 2.047\n",
      "[12,   200] loss: 2.075\n",
      "[13,    50] loss: 2.006\n",
      "[13,   100] loss: 2.050\n",
      "[13,   150] loss: 2.039\n",
      "[13,   200] loss: 2.051\n",
      "[14,    50] loss: 2.022\n",
      "[14,   100] loss: 2.024\n",
      "[14,   150] loss: 2.034\n",
      "[14,   200] loss: 2.007\n",
      "[15,    50] loss: 2.033\n",
      "[15,   100] loss: 1.985\n",
      "[15,   150] loss: 2.010\n",
      "[15,   200] loss: 2.032\n",
      "[16,    50] loss: 2.006\n",
      "[16,   100] loss: 2.021\n",
      "[16,   150] loss: 1.963\n",
      "[16,   200] loss: 2.043\n",
      "[17,    50] loss: 1.969\n",
      "[17,   100] loss: 1.994\n",
      "[17,   150] loss: 1.990\n",
      "[17,   200] loss: 2.000\n",
      "[18,    50] loss: 1.985\n",
      "[18,   100] loss: 1.970\n",
      "[18,   150] loss: 1.977\n",
      "[18,   200] loss: 2.021\n",
      "[19,    50] loss: 1.968\n",
      "[19,   100] loss: 1.993\n",
      "[19,   150] loss: 1.985\n",
      "[19,   200] loss: 1.951\n",
      "[20,    50] loss: 1.991\n",
      "[20,   100] loss: 1.957\n",
      "[20,   150] loss: 1.964\n",
      "[20,   200] loss: 1.963\n",
      "[21,    50] loss: 1.954\n",
      "[21,   100] loss: 1.944\n",
      "[21,   150] loss: 1.964\n",
      "[21,   200] loss: 2.018\n",
      "[22,    50] loss: 1.962\n",
      "[22,   100] loss: 1.940\n",
      "[22,   150] loss: 1.940\n",
      "[22,   200] loss: 1.959\n",
      "[23,    50] loss: 1.962\n",
      "[23,   100] loss: 1.912\n",
      "[23,   150] loss: 1.937\n",
      "[23,   200] loss: 1.971\n",
      "[24,    50] loss: 1.939\n",
      "[24,   100] loss: 1.968\n",
      "[24,   150] loss: 1.908\n",
      "[24,   200] loss: 1.956\n",
      "[25,    50] loss: 1.910\n",
      "[25,   100] loss: 1.948\n",
      "[25,   150] loss: 1.937\n",
      "[25,   200] loss: 1.941\n",
      "[26,    50] loss: 1.873\n",
      "[26,   100] loss: 1.954\n",
      "[26,   150] loss: 1.927\n",
      "[26,   200] loss: 1.942\n",
      "[27,    50] loss: 1.901\n",
      "[27,   100] loss: 1.920\n",
      "[27,   150] loss: 1.941\n",
      "[27,   200] loss: 1.946\n",
      "[28,    50] loss: 1.934\n",
      "[28,   100] loss: 1.923\n",
      "[28,   150] loss: 1.939\n",
      "[28,   200] loss: 1.896\n",
      "[29,    50] loss: 1.884\n",
      "[29,   100] loss: 1.915\n",
      "[29,   150] loss: 1.924\n",
      "[29,   200] loss: 1.963\n",
      "[30,    50] loss: 1.887\n",
      "[30,   100] loss: 1.932\n",
      "[30,   150] loss: 1.905\n",
      "[30,   200] loss: 1.870\n",
      "[31,    50] loss: 1.871\n",
      "[31,   100] loss: 1.895\n",
      "[31,   150] loss: 1.912\n",
      "[31,   200] loss: 1.891\n",
      "[32,    50] loss: 1.910\n",
      "[32,   100] loss: 1.906\n",
      "[32,   150] loss: 1.870\n",
      "[32,   200] loss: 1.907\n",
      "[33,    50] loss: 1.910\n",
      "[33,   100] loss: 1.872\n",
      "[33,   150] loss: 1.882\n",
      "[33,   200] loss: 1.876\n",
      "[34,    50] loss: 1.842\n",
      "[34,   100] loss: 1.842\n",
      "[34,   150] loss: 1.905\n",
      "[34,   200] loss: 1.928\n",
      "[35,    50] loss: 1.876\n",
      "[35,   100] loss: 1.887\n",
      "[35,   150] loss: 1.890\n",
      "[35,   200] loss: 1.921\n",
      "[36,    50] loss: 1.858\n",
      "[36,   100] loss: 1.879\n",
      "[36,   150] loss: 1.895\n",
      "[36,   200] loss: 1.848\n",
      "[37,    50] loss: 1.863\n",
      "[37,   100] loss: 1.908\n",
      "[37,   150] loss: 1.841\n",
      "[37,   200] loss: 1.866\n",
      "[38,    50] loss: 1.868\n",
      "[38,   100] loss: 1.875\n",
      "[38,   150] loss: 1.846\n",
      "[38,   200] loss: 1.907\n",
      "[39,    50] loss: 1.912\n",
      "[39,   100] loss: 1.857\n",
      "[39,   150] loss: 1.884\n",
      "[39,   200] loss: 1.851\n",
      "[40,    50] loss: 1.856\n",
      "[40,   100] loss: 1.874\n",
      "[40,   150] loss: 1.883\n",
      "[40,   200] loss: 1.837\n",
      "[41,    50] loss: 1.856\n",
      "[41,   100] loss: 1.874\n",
      "[41,   150] loss: 1.859\n",
      "[41,   200] loss: 1.860\n",
      "[42,    50] loss: 1.832\n",
      "[42,   100] loss: 1.870\n",
      "[42,   150] loss: 1.858\n",
      "[42,   200] loss: 1.834\n",
      "[43,    50] loss: 1.839\n",
      "[43,   100] loss: 1.877\n",
      "[43,   150] loss: 1.894\n",
      "[43,   200] loss: 1.831\n",
      "[44,    50] loss: 1.852\n",
      "[44,   100] loss: 1.822\n",
      "[44,   150] loss: 1.852\n",
      "[44,   200] loss: 1.833\n",
      "[45,    50] loss: 1.853\n",
      "[45,   100] loss: 1.821\n",
      "[45,   150] loss: 1.829\n",
      "[45,   200] loss: 1.853\n",
      "[46,    50] loss: 1.821\n",
      "[46,   100] loss: 1.852\n",
      "[46,   150] loss: 1.911\n",
      "[46,   200] loss: 1.796\n",
      "[47,    50] loss: 1.851\n",
      "[47,   100] loss: 1.837\n",
      "[47,   150] loss: 1.852\n",
      "[47,   200] loss: 1.840\n",
      "[48,    50] loss: 1.852\n",
      "[48,   100] loss: 1.842\n",
      "[48,   150] loss: 1.842\n",
      "[48,   200] loss: 1.842\n",
      "[49,    50] loss: 1.824\n",
      "[49,   100] loss: 1.809\n",
      "[49,   150] loss: 1.848\n",
      "[49,   200] loss: 1.801\n",
      "[50,    50] loss: 1.821\n",
      "[50,   100] loss: 1.859\n",
      "[50,   150] loss: 1.809\n",
      "[50,   200] loss: 1.841\n",
      "[51,    50] loss: 1.835\n",
      "[51,   100] loss: 1.824\n",
      "[51,   150] loss: 1.791\n",
      "[51,   200] loss: 1.853\n",
      "[52,    50] loss: 1.806\n",
      "[52,   100] loss: 1.804\n",
      "[52,   150] loss: 1.830\n",
      "[52,   200] loss: 1.853\n",
      "[53,    50] loss: 1.838\n",
      "[53,   100] loss: 1.847\n",
      "[53,   150] loss: 1.817\n",
      "[53,   200] loss: 1.795\n",
      "[54,    50] loss: 1.783\n",
      "[54,   100] loss: 1.835\n",
      "[54,   150] loss: 1.855\n",
      "[54,   200] loss: 1.836\n",
      "[55,    50] loss: 1.822\n",
      "[55,   100] loss: 1.805\n",
      "[55,   150] loss: 1.830\n",
      "[55,   200] loss: 1.809\n",
      "[56,    50] loss: 1.802\n",
      "[56,   100] loss: 1.794\n",
      "[56,   150] loss: 1.844\n",
      "[56,   200] loss: 1.800\n",
      "[57,    50] loss: 1.818\n",
      "[57,   100] loss: 1.794\n",
      "[57,   150] loss: 1.796\n",
      "[57,   200] loss: 1.813\n",
      "[58,    50] loss: 1.802\n",
      "[58,   100] loss: 1.807\n",
      "[58,   150] loss: 1.825\n",
      "[58,   200] loss: 1.834\n",
      "[59,    50] loss: 1.811\n",
      "[59,   100] loss: 1.800\n",
      "[59,   150] loss: 1.849\n",
      "[59,   200] loss: 1.811\n",
      "[60,    50] loss: 1.835\n",
      "[60,   100] loss: 1.772\n",
      "[60,   150] loss: 1.790\n",
      "[60,   200] loss: 1.808\n",
      "[61,    50] loss: 1.814\n",
      "[61,   100] loss: 1.805\n",
      "[61,   150] loss: 1.758\n",
      "[61,   200] loss: 1.846\n",
      "[62,    50] loss: 1.793\n",
      "[62,   100] loss: 1.787\n",
      "[62,   150] loss: 1.821\n",
      "[62,   200] loss: 1.833\n",
      "[63,    50] loss: 1.828\n",
      "[63,   100] loss: 1.768\n",
      "[63,   150] loss: 1.825\n",
      "[63,   200] loss: 1.779\n",
      "[64,    50] loss: 1.796\n",
      "[64,   100] loss: 1.820\n",
      "[64,   150] loss: 1.832\n",
      "[64,   200] loss: 1.780\n",
      "[65,    50] loss: 1.787\n",
      "[65,   100] loss: 1.836\n",
      "[65,   150] loss: 1.830\n",
      "[65,   200] loss: 1.794\n",
      "[66,    50] loss: 1.822\n",
      "[66,   100] loss: 1.791\n",
      "[66,   150] loss: 1.765\n",
      "[66,   200] loss: 1.808\n",
      "[67,    50] loss: 1.805\n",
      "[67,   100] loss: 1.782\n",
      "[67,   150] loss: 1.782\n",
      "[67,   200] loss: 1.819\n",
      "[68,    50] loss: 1.824\n",
      "[68,   100] loss: 1.765\n",
      "[68,   150] loss: 1.780\n",
      "[68,   200] loss: 1.799\n",
      "[69,    50] loss: 1.775\n",
      "[69,   100] loss: 1.798\n",
      "[69,   150] loss: 1.780\n",
      "[69,   200] loss: 1.822\n",
      "[70,    50] loss: 1.832\n",
      "[70,   100] loss: 1.766\n",
      "[70,   150] loss: 1.813\n",
      "[70,   200] loss: 1.803\n",
      "[71,    50] loss: 1.821\n",
      "[71,   100] loss: 1.815\n",
      "[71,   150] loss: 1.770\n",
      "[71,   200] loss: 1.785\n",
      "[72,    50] loss: 1.802\n",
      "[72,   100] loss: 1.812\n",
      "[72,   150] loss: 1.775\n",
      "[72,   200] loss: 1.769\n",
      "[73,    50] loss: 1.756\n",
      "[73,   100] loss: 1.792\n",
      "[73,   150] loss: 1.784\n",
      "[73,   200] loss: 1.797\n",
      "[74,    50] loss: 1.775\n",
      "[74,   100] loss: 1.755\n",
      "[74,   150] loss: 1.837\n",
      "[74,   200] loss: 1.774\n",
      "[75,    50] loss: 1.799\n",
      "[75,   100] loss: 1.803\n",
      "[75,   150] loss: 1.755\n",
      "[75,   200] loss: 1.800\n",
      "[76,    50] loss: 1.805\n",
      "[76,   100] loss: 1.817\n",
      "[76,   150] loss: 1.767\n",
      "[76,   200] loss: 1.775\n",
      "[77,    50] loss: 1.772\n",
      "[77,   100] loss: 1.793\n",
      "[77,   150] loss: 1.788\n",
      "[77,   200] loss: 1.778\n",
      "[78,    50] loss: 1.801\n",
      "[78,   100] loss: 1.762\n",
      "[78,   150] loss: 1.769\n",
      "[78,   200] loss: 1.763\n",
      "[79,    50] loss: 1.784\n",
      "[79,   100] loss: 1.807\n",
      "[79,   150] loss: 1.812\n",
      "[79,   200] loss: 1.740\n",
      "[80,    50] loss: 1.773\n",
      "[80,   100] loss: 1.765\n",
      "[80,   150] loss: 1.789\n",
      "[80,   200] loss: 1.758\n",
      "[81,    50] loss: 1.753\n",
      "[81,   100] loss: 1.789\n",
      "[81,   150] loss: 1.779\n",
      "[81,   200] loss: 1.808\n",
      "[82,    50] loss: 1.789\n",
      "[82,   100] loss: 1.750\n",
      "[82,   150] loss: 1.793\n",
      "[82,   200] loss: 1.760\n",
      "[83,    50] loss: 1.773\n",
      "[83,   100] loss: 1.775\n",
      "[83,   150] loss: 1.754\n",
      "[83,   200] loss: 1.772\n",
      "[84,    50] loss: 1.764\n",
      "[84,   100] loss: 1.772\n",
      "[84,   150] loss: 1.733\n",
      "[84,   200] loss: 1.793\n",
      "[85,    50] loss: 1.815\n",
      "[85,   100] loss: 1.760\n",
      "[85,   150] loss: 1.804\n",
      "[85,   200] loss: 1.773\n",
      "[86,    50] loss: 1.773\n",
      "[86,   100] loss: 1.771\n",
      "[86,   150] loss: 1.754\n",
      "[86,   200] loss: 1.767\n",
      "[87,    50] loss: 1.760\n",
      "[87,   100] loss: 1.740\n",
      "[87,   150] loss: 1.789\n",
      "[87,   200] loss: 1.753\n",
      "[88,    50] loss: 1.745\n",
      "[88,   100] loss: 1.746\n",
      "[88,   150] loss: 1.754\n",
      "[88,   200] loss: 1.796\n",
      "[89,    50] loss: 1.772\n",
      "[89,   100] loss: 1.765\n",
      "[89,   150] loss: 1.760\n",
      "[89,   200] loss: 1.770\n",
      "[90,    50] loss: 1.759\n",
      "[90,   100] loss: 1.759\n",
      "[90,   150] loss: 1.757\n",
      "[90,   200] loss: 1.781\n",
      "[91,    50] loss: 1.758\n",
      "[91,   100] loss: 1.731\n",
      "[91,   150] loss: 1.763\n",
      "[91,   200] loss: 1.783\n",
      "[92,    50] loss: 1.749\n",
      "[92,   100] loss: 1.744\n",
      "[92,   150] loss: 1.776\n",
      "[92,   200] loss: 1.748\n",
      "[93,    50] loss: 1.752\n",
      "[93,   100] loss: 1.759\n",
      "[93,   150] loss: 1.766\n",
      "[93,   200] loss: 1.780\n",
      "[94,    50] loss: 1.730\n",
      "[94,   100] loss: 1.763\n",
      "[94,   150] loss: 1.759\n",
      "[94,   200] loss: 1.749\n",
      "[95,    50] loss: 1.801\n",
      "[95,   100] loss: 1.754\n",
      "[95,   150] loss: 1.689\n",
      "[95,   200] loss: 1.767\n",
      "[96,    50] loss: 1.778\n",
      "[96,   100] loss: 1.756\n",
      "[96,   150] loss: 1.744\n",
      "[96,   200] loss: 1.741\n",
      "[97,    50] loss: 1.746\n",
      "[97,   100] loss: 1.789\n",
      "[97,   150] loss: 1.751\n",
      "[97,   200] loss: 1.748\n",
      "[98,    50] loss: 1.764\n",
      "[98,   100] loss: 1.721\n",
      "[98,   150] loss: 1.773\n",
      "[98,   200] loss: 1.737\n",
      "[99,    50] loss: 1.740\n",
      "[99,   100] loss: 1.741\n",
      "[99,   150] loss: 1.806\n",
      "[99,   200] loss: 1.754\n",
      "[100,    50] loss: 1.761\n",
      "[100,   100] loss: 1.727\n",
      "[100,   150] loss: 1.732\n",
      "[100,   200] loss: 1.779\n",
      "Accuracy: 35 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3523809523809524"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train completely\n",
    "\n",
    "#High level inputs\n",
    "use_gpu         = False\n",
    "seed            = 0\n",
    "\n",
    "#Training inputs\n",
    "batch_size      = 8\n",
    "n_epochs        = 100\n",
    "step_size       = s_selected\n",
    "momentum        = m_selected\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#Network architecture inputs\n",
    "n_hidden_layers = 3\n",
    "use_batchnorm   = True\n",
    "use_xavier      = True\n",
    "net = CNN(n_hidden_layers=n_hidden_layers, activation=a_selected, stride=(ks_selected,ks_selected), dim=(kd_selected,kd_selected), pool_stride=(1,1), pool_dim=(2,2), use_batchnorm=True, use_xavier=True, max_channels=c_selected, max_pool = True)\n",
    "\n",
    "#Visualization inputs\n",
    "show_loss_every = 50\n",
    "net = train(net, n_epochs, trainloader, use_gpu, step_size, momentum, show_loss_every)\n",
    "test(net, testloader, use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0eAC2AURITb",
    "outputId": "3d5ea562-cef6-49a2-d62d-ab8d24cf10a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activation:  ReLU() , max channels:  15 , momentum:  0.667284844044465 , step size:  0.0015199110829529332 , kernel dim:  2 , kernel stride:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.319\n",
      "[1,   100] loss: 2.304\n",
      "[1,   150] loss: 2.297\n",
      "[1,   200] loss: 2.303\n",
      "[2,    50] loss: 2.296\n",
      "[2,   100] loss: 2.292\n",
      "[2,   150] loss: 2.292\n",
      "[2,   200] loss: 2.283\n",
      "[3,    50] loss: 2.279\n",
      "[3,   100] loss: 2.279\n",
      "[3,   150] loss: 2.298\n",
      "[3,   200] loss: 2.283\n",
      "Accuracy: 12 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  45 , momentum:  0.2606286668926199 , step size:  0.30538555088334157 , kernel dim:  2 , kernel stride:  2\n",
      "[1,    50] loss: 2.287\n",
      "[1,   100] loss: 2.270\n",
      "[1,   150] loss: 2.281\n",
      "[1,   200] loss: 2.249\n",
      "[2,    50] loss: 2.258\n",
      "[2,   100] loss: 2.234\n",
      "[2,   150] loss: 2.252\n",
      "[2,   200] loss: 2.218\n",
      "[3,    50] loss: 2.243\n",
      "[3,   100] loss: 2.240\n",
      "[3,   150] loss: 2.211\n",
      "[3,   200] loss: 2.233\n",
      "Accuracy: 19 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  55 , momentum:  0.1372513505241385 , step size:  0.0023101297000831605 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.281\n",
      "[1,   100] loss: 2.242\n",
      "[1,   150] loss: 2.209\n",
      "[1,   200] loss: 2.175\n",
      "[2,    50] loss: 2.179\n",
      "[2,   100] loss: 2.154\n",
      "[2,   150] loss: 2.182\n",
      "[2,   200] loss: 2.162\n",
      "[3,    50] loss: 2.148\n",
      "[3,   100] loss: 2.123\n",
      "[3,   150] loss: 2.130\n",
      "[3,   200] loss: 2.157\n",
      "Accuracy: 35 %\n",
      "\n",
      "Activation:  ReLU() , max channels:  30 , momentum:  0.38310289154914323 , step size:  0.1747528400007685 , kernel dim:  2 , kernel stride:  2\n",
      "[1,    50] loss: 2.299\n",
      "[1,   100] loss: 2.300\n",
      "[1,   150] loss: 2.235\n",
      "[1,   200] loss: 2.270\n",
      "[2,    50] loss: 2.206\n",
      "[2,   100] loss: 2.251\n",
      "[2,   150] loss: 2.228\n",
      "[2,   200] loss: 2.232\n",
      "[3,    50] loss: 2.210\n",
      "[3,   100] loss: 2.192\n",
      "[3,   150] loss: 2.208\n",
      "[3,   200] loss: 2.195\n",
      "Accuracy: 24 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  35 , momentum:  0.667284844044465 , step size:  0.093260334688322 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.363\n",
      "[1,   100] loss: 2.386\n",
      "[1,   150] loss: 2.334\n",
      "[1,   200] loss: 2.359\n",
      "[2,    50] loss: 2.356\n",
      "[2,   100] loss: 2.341\n",
      "[2,   150] loss: 2.361\n",
      "[2,   200] loss: 2.356\n",
      "[3,    50] loss: 2.334\n",
      "[3,   100] loss: 2.354\n",
      "[3,   150] loss: 2.371\n",
      "[3,   200] loss: 2.349\n",
      "Accuracy: 6 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  35 , momentum:  0.1372513505241385 , step size:  0.002009233002565048 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.301\n",
      "[1,   100] loss: 2.249\n",
      "[1,   150] loss: 2.241\n",
      "[1,   200] loss: 2.206\n",
      "[2,    50] loss: 2.200\n",
      "[2,   100] loss: 2.183\n",
      "[2,   150] loss: 2.212\n",
      "[2,   200] loss: 2.200\n",
      "[3,    50] loss: 2.154\n",
      "[3,   100] loss: 2.168\n",
      "[3,   150] loss: 2.185\n",
      "[3,   200] loss: 2.163\n",
      "Accuracy: 32 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  25 , momentum:  0.8801068635197443 , step size:  0.5336699231206312 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.343\n",
      "[1,   100] loss: 2.336\n",
      "[1,   150] loss: 2.374\n",
      "[1,   200] loss: 2.369\n",
      "[2,    50] loss: 2.361\n",
      "[2,   100] loss: 2.349\n",
      "[2,   150] loss: 2.346\n",
      "[2,   200] loss: 2.374\n",
      "[3,    50] loss: 2.344\n",
      "[3,   100] loss: 2.354\n",
      "[3,   150] loss: 2.351\n",
      "[3,   200] loss: 2.364\n",
      "Accuracy: 5 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  10 , momentum:  0.1372513505241385 , step size:  0.001 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.313\n",
      "[1,   100] loss: 2.291\n",
      "[1,   150] loss: 2.290\n",
      "[1,   200] loss: 2.284\n",
      "[2,    50] loss: 2.262\n",
      "[2,   100] loss: 2.245\n",
      "[2,   150] loss: 2.245\n",
      "[2,   200] loss: 2.233\n",
      "[3,    50] loss: 2.237\n",
      "[3,   100] loss: 2.235\n",
      "[3,   150] loss: 2.233\n",
      "[3,   200] loss: 2.223\n",
      "Accuracy: 27 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  25 , momentum:  0.7652705837982595 , step size:  0.005336699231206312 , kernel dim:  2 , kernel stride:  1\n",
      "[1,    50] loss: 2.290\n",
      "[1,   100] loss: 2.298\n",
      "[1,   150] loss: 2.284\n",
      "[1,   200] loss: 2.257\n",
      "[2,    50] loss: 2.240\n",
      "[2,   100] loss: 2.249\n",
      "[2,   150] loss: 2.273\n",
      "[2,   200] loss: 2.283\n",
      "[3,    50] loss: 2.248\n",
      "[3,   100] loss: 2.233\n",
      "[3,   150] loss: 2.208\n",
      "[3,   200] loss: 2.214\n",
      "Accuracy: 20 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  30 , momentum:  0.5808872741477971 , step size:  0.24770763559917114 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.364\n",
      "[1,   100] loss: 2.361\n",
      "[1,   150] loss: 2.359\n",
      "[1,   200] loss: 2.359\n",
      "[2,    50] loss: 2.366\n",
      "[2,   100] loss: 2.374\n",
      "[2,   150] loss: 2.351\n",
      "[2,   200] loss: 2.356\n",
      "[3,    50] loss: 2.374\n",
      "[3,   100] loss: 2.384\n",
      "[3,   150] loss: 2.339\n",
      "[3,   200] loss: 2.344\n",
      "Accuracy: 10 %\n",
      "\n",
      "Activation:  ReLU() , max channels:  30 , momentum:  0.2270839709579695 , step size:  0.02848035868435802 , kernel dim:  2 , kernel stride:  1\n",
      "[1,    50] loss: 2.303\n",
      "[1,   100] loss: 2.303\n",
      "[1,   150] loss: 2.267\n",
      "[1,   200] loss: 2.267\n",
      "[2,    50] loss: 2.250\n",
      "[2,   100] loss: 2.232\n",
      "[2,   150] loss: 2.232\n",
      "[2,   200] loss: 2.212\n",
      "[3,    50] loss: 2.227\n",
      "[3,   100] loss: 2.169\n",
      "[3,   150] loss: 2.174\n",
      "[3,   200] loss: 2.173\n",
      "Accuracy: 22 %\n",
      "\n",
      "Activation:  ReLU() , max channels:  35 , momentum:  0.2666968776004438 , step size:  0.037649358067924674 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.350\n",
      "[1,   100] loss: 2.311\n",
      "[1,   150] loss: 2.357\n",
      "[1,   200] loss: 2.344\n",
      "[2,    50] loss: 2.335\n",
      "[2,   100] loss: 2.303\n",
      "[2,   150] loss: 2.308\n",
      "[2,   200] loss: 2.375\n",
      "[3,    50] loss: 2.329\n",
      "[3,   100] loss: 2.302\n",
      "[3,   150] loss: 2.382\n",
      "[3,   200] loss: 2.369\n",
      "Accuracy: 13 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  20 , momentum:  0.672041057147487 , step size:  0.7054802310718645 , kernel dim:  2 , kernel stride:  2\n",
      "[1,    50] loss: 2.320\n",
      "[1,   100] loss: 2.290\n",
      "[1,   150] loss: 2.273\n",
      "[1,   200] loss: 2.250\n",
      "[2,    50] loss: 2.253\n",
      "[2,   100] loss: 2.272\n",
      "[2,   150] loss: 2.281\n",
      "[2,   200] loss: 2.240\n",
      "[3,    50] loss: 2.246\n",
      "[3,   100] loss: 2.279\n",
      "[3,   150] loss: 2.293\n",
      "[3,   200] loss: 2.292\n",
      "Accuracy: 14 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  90 , momentum:  0.31648791871320214 , step size:  0.001 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.310\n",
      "[1,   100] loss: 2.266\n",
      "[1,   150] loss: 2.261\n",
      "[1,   200] loss: 2.245\n",
      "[2,    50] loss: 2.238\n",
      "[2,   100] loss: 2.235\n",
      "[2,   150] loss: 2.227\n",
      "[2,   200] loss: 2.203\n",
      "[3,    50] loss: 2.193\n",
      "[3,   100] loss: 2.206\n",
      "[3,   150] loss: 2.189\n",
      "[3,   200] loss: 2.164\n",
      "Accuracy: 23 %\n",
      "\n",
      "Activation:  ReLU() , max channels:  80 , momentum:  0.8784926238627397 , step size:  0.007054802310718645 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.350\n",
      "[1,   100] loss: 2.361\n",
      "[1,   150] loss: 2.364\n",
      "[1,   200] loss: 2.366\n",
      "[2,    50] loss: 2.361\n",
      "[2,   100] loss: 2.374\n",
      "[2,   150] loss: 2.363\n",
      "[2,   200] loss: 2.358\n",
      "[3,    50] loss: 2.325\n",
      "[3,   100] loss: 2.354\n",
      "[3,   150] loss: 2.329\n",
      "[3,   200] loss: 2.293\n",
      "Accuracy: 13 %\n",
      "\n",
      "Activation:  ReLU() , max channels:  95 , momentum:  0.919531435498915 , step size:  0.0011497569953977356 , kernel dim:  2 , kernel stride:  2\n",
      "[1,    50] loss: 2.308\n",
      "[1,   100] loss: 2.242\n",
      "[1,   150] loss: 2.216\n",
      "[1,   200] loss: 2.218\n",
      "[2,    50] loss: 2.199\n",
      "[2,   100] loss: 2.190\n",
      "[2,   150] loss: 2.193\n",
      "[2,   200] loss: 2.148\n",
      "[3,    50] loss: 2.157\n",
      "[3,   100] loss: 2.122\n",
      "[3,   150] loss: 2.168\n",
      "[3,   200] loss: 2.178\n",
      "Accuracy: 28 %\n",
      "\n",
      "Activation:  ReLU() , max channels:  50 , momentum:  0.5391182349867721 , step size:  0.1873817422860385 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.363\n",
      "[1,   100] loss: 2.346\n",
      "[1,   150] loss: 2.339\n",
      "[1,   200] loss: 2.389\n",
      "[2,    50] loss: 2.359\n",
      "[2,   100] loss: 2.371\n",
      "[2,   150] loss: 2.356\n",
      "[2,   200] loss: 2.351\n",
      "[3,    50] loss: 2.359\n",
      "[3,   100] loss: 2.364\n",
      "[3,   150] loss: 2.364\n",
      "[3,   200] loss: 2.364\n",
      "Accuracy: 9 %\n",
      "\n",
      "Activation:  ReLU() , max channels:  25 , momentum:  0.5640643603339318 , step size:  0.023101297000831605 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.354\n",
      "[1,   100] loss: 2.339\n",
      "[1,   150] loss: 2.354\n",
      "[1,   200] loss: 2.384\n",
      "[2,    50] loss: 2.361\n",
      "[2,   100] loss: 2.313\n",
      "[2,   150] loss: 2.322\n",
      "[2,   200] loss: 2.289\n",
      "[3,    50] loss: 2.277\n",
      "[3,   100] loss: 2.292\n",
      "[3,   150] loss: 2.279\n",
      "[3,   200] loss: 2.346\n",
      "Accuracy: 12 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  90 , momentum:  0.5390735467669154 , step size:  0.004037017258596553 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.369\n",
      "[1,   100] loss: 2.319\n",
      "[1,   150] loss: 2.300\n",
      "[1,   200] loss: 2.252\n",
      "[2,    50] loss: 2.289\n",
      "[2,   100] loss: 2.266\n",
      "[2,   150] loss: 2.270\n",
      "[2,   200] loss: 2.262\n",
      "[3,    50] loss: 2.260\n",
      "[3,   100] loss: 2.245\n",
      "[3,   150] loss: 2.245\n",
      "[3,   200] loss: 2.226\n",
      "Accuracy: 19 %\n",
      "\n",
      "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  15 , momentum:  0.5640643603339318 , step size:  0.24770763559917114 , kernel dim:  3 , kernel stride:  1\n",
      "[1,    50] loss: 2.365\n",
      "[1,   100] loss: 2.369\n",
      "[1,   150] loss: 2.376\n",
      "[1,   200] loss: 2.366\n",
      "[2,    50] loss: 2.379\n",
      "[2,   100] loss: 2.381\n",
      "[2,   150] loss: 2.336\n",
      "[2,   200] loss: 2.369\n",
      "[3,    50] loss: 2.379\n",
      "[3,   100] loss: 2.341\n",
      "[3,   150] loss: 2.369\n",
      "[3,   200] loss: 2.356\n",
      "Accuracy: 13 %\n"
     ]
    }
   ],
   "source": [
    "# batch_size      = 8\n",
    "# trainloader     = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "# testloader      = DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# n_epochs        = 3\n",
    "# step_size       = np.random.choice(np.logspace(-3, 0, 100), 100)\n",
    "# momentum        = np.random.random(100)\n",
    "\n",
    "# #Fixed network architecture inputs\n",
    "# n_hidden_layers = 3\n",
    "# use_batchnorm   = True\n",
    "# use_xavier      = True\n",
    "\n",
    "# #Hyperparameters\n",
    "# activation = np.array([nn.ReLU(), nn.LeakyReLU()])\n",
    "# max_channels = np.arange(5, 100, 5)\n",
    "# kernel_dim = np.array([2,3])\n",
    "# kernel_stride = np.array([1,2])\n",
    "\n",
    "# num_search = 20\n",
    "# a_search = activation[np.random.randint(0,len(activation),num_search)]\n",
    "# c_search = max_channels[np.random.randint(0,len(max_channels),num_search)]\n",
    "# s_search = step_size[np.random.randint(0,len(step_size), num_search)]\n",
    "# m_search = momentum[np.random.randint(0,len(momentum), num_search)]\n",
    "# kd_search = kernel_dim[np.random.randint(0,len(kernel_dim), num_search)]\n",
    "# ks_search = kernel_stride[np.random.randint(0,len(kernel_stride), num_search)]\n",
    "\n",
    "# val_accuracy = np.empty(num_search)\n",
    "# for i in range(num_search):\n",
    "#   a = a_search[i]\n",
    "#   c = c_search[i]\n",
    "#   s = s_search[i]\n",
    "#   m = m_search[i]\n",
    "#   kd = kd_search[i]\n",
    "#   if kd == 3: ks = 1\n",
    "#   else: ks = ks_search[i]\n",
    "#   print()\n",
    "#   print('Activation: ', str(a), ', max channels: ', str(c), ', momentum: ', str(m), ', step size: ', str(s), ', kernel dim: ', str(kd), ', kernel stride: ', str(ks))\n",
    "#   net = CNN(n_hidden_layers=n_hidden_layers, activation=a, stride=(ks,ks), dim=(kd,kd), pool_stride=(1,1), pool_dim=(2,2), use_batchnorm=True, use_xavier=True, max_channels=c, max_pool = True)\n",
    "#   net = train(net, n_epochs, trainloader, use_gpu, s, m, show_loss_every=50)\n",
    "#   val_accuracy[i] = test(net, testloader, use_gpu) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOcAMdbZf4gm"
   },
   "source": [
    "Ex. of validation:\n",
    "Activation:  ReLU() , max channels:  40 , momentum:  0.5938300983792906 , step size:  0.021544346900318846 , kernel dim:  2 , kernel stride:  1\n",
    "Accuracy: 7 %\n",
    "\n",
    "Activation:  ReLU() , max channels:  5 , momentum:  0.1377248457779756 , step size:  0.12328467394420659 , kernel dim:  2 , kernel stride:  2\n",
    "Accuracy: 8 %\n",
    "\n",
    "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  30 , momentum:  0.5376233556508321 , step size:  0.01873817422860384 , kernel dim:  2 , kernel stride:  2\n",
    "Accuracy: 18 %\n",
    "\n",
    "Activation:  ReLU() , max channels:  55 , momentum:  0.6124252490967343 , step size:  0.026560877829466867 , kernel dim:  2 , kernel stride:  2\n",
    "Accuracy: 18 %\n",
    "\n",
    "Activation:  ReLU() , max channels:  60 , momentum:  0.46492216869766234 , step size:  0.04328761281083059 , kernel dim:  2 , kernel stride:  2\n",
    "Accuracy: 20 %\n",
    "\n",
    "Activation:  ReLU() , max channels:  35 , momentum:  0.46492216869766234 , step size:  0.010722672220103232 , kernel dim:  2 , kernel stride:  2\n",
    "Accuracy: 18 %\n",
    "\n",
    "Activation:  ReLU() , max channels:  10 , momentum:  0.771146097054588 , step size:  0.7054802310718645 , kernel dim:  2 , kernel stride:  2\n",
    "Accuracy: 7 %\n",
    "\n",
    "Activation:  LeakyReLU(negative_slope=0.01) , max channels:  70 , momentum:  0.46492216869766234 , step size:  0.0016297508346206436 , kernel dim:  2 , kernel stride:  1\n",
    "Accuracy: 22 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTyfNjRAYHst"
   },
   "outputs": [],
   "source": [
    "# i_selected = np.argmax(val_accuracy)\n",
    "# # a_selected = nn.ReLU() #a_search[i_selected]\n",
    "# # c_selected = c_search[i_selected]\n",
    "# # s_selected = s_search[i_selected]\n",
    "# # m_selected = m_search[i_selected]\n",
    "# # kd_selected = kd_search[i_selected]\n",
    "# # ks_selected = ks_search[i_selected]\n",
    "\n",
    "# a_selected = nn.LeakyReLU() #a_search[i_selected]\n",
    "# c_selected = 55\n",
    "# s_selected = 0.0023101297000831605\n",
    "# m_selected = 0.1372513505241385\n",
    "# kd_selected = 3\n",
    "# ks_selected = 1\n",
    "# # LeakyReLU(negative_slope=0.01) , max channels:  55 , momentum:  0.1372513505241385 , step size:  0.0023101297000831605 , kernel dim:  3 , kernel stride:  1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4rodRhulBIfw",
    "outputId": "95ae5817-fe32-4df9-91a9-c7e849030028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 8], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
